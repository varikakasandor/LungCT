{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fundamentals\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "#System\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"/mnt/idms/PROJECTS/Lung/LungCT/Super-resolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 80\n",
    "learning_rate = 0.05\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets_from_case(case_path, idx):\n",
    "    \"\"\"\n",
    "    Ideas for improvement:\n",
    "    -Why do we have to double the number of slices? Couldn't we predict not 1, but more in-between?\n",
    "    -Why to take just the neighbouring ones?\n",
    "    -Now the triplets are (1,2,3),(4,5,6),(7,8,9)..., but could get more triplets by (1,2,3), (2,3,4), (3,4,5)...\n",
    "    \"\"\"\n",
    "    npz=np.load(case_path)\n",
    "    ct=npz[npz.files[0]]\n",
    "    ct=np.moveaxis(ct,-1,0)\n",
    "    ct=ct[:(len(ct)-(len(ct)%3))]\n",
    "    case_triplets=np.asarray(np.split(ct,len(ct)//3))\n",
    "    case_triplets=(case_triplets-np.min(case_triplets))/(np.max(case_triplets)-np.min(case_triplets))\n",
    "    print(f\"Case {idx+1} is done\")\n",
    "    return case_triplets\n",
    "    \n",
    "def create_all_triplets(parallel_jobs=mp.cpu_count(), max_cases=None):#, save=False):\n",
    "    ct_path=\"/mnt/idms/PROJECTS/Lung/Tudo-Ulyssys-Unzipped\"\n",
    "    case_paths=list(glob.glob(f'{ct_path}/*/*.npz'))\n",
    "    if max_cases:\n",
    "        case_paths=case_paths[:max_cases]\n",
    "    with mp.Pool(parallel_jobs) as pool: #get_context(\"spawn\")\n",
    "        triplets = pool.starmap(create_triplets_from_case, zip(case_paths,list(range(len(case_paths)))))\n",
    "    pool.join() #Maybe helps\n",
    "\n",
    "    \"\"\"if save:\n",
    "        with h5py.File(f\"{folder}/data/input_data.h5\", 'w') as h5f:\n",
    "            h5f.create_dataset('triplets_per_patients', data=np.asarray(triplets))#, dtype=object))\n",
    "    \"\"\"\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 3 is done\n",
      "Case 5 is done\n",
      "Case 4 is done\n",
      "Case 2 is done\n",
      "Case 1 is done\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "triplets=create_all_triplets(max_cases=5, parallel_jobs=5) #VEEEERY SLOW WITH MORE MAX_CASES, 5000 DOESN'T EVEN RUN\n",
    "#I COULDN'T FIND THE EXACT CAUSE, BUT IT CAN DEADLOCK WITH TOO MANY PARALLEL_JOBS AND MAX_CASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triplets(Dataset):\n",
    "    \n",
    "    def __init__(self, data, triplets_per_case=20, transform=None):\n",
    "        self.data = []\n",
    "        self.len=len(data)*triplets_per_case\n",
    "        for case in data:\n",
    "            self.data+=list(case[np.random.choice(case.shape[0], triplets_per_case, replace=False),:,:,:])\n",
    "        self.data=np.asarray(self.data)\n",
    "        self.transform = transform\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        xs=(self.data[idx,0,:,:],self.data[idx,2,:,:])\n",
    "        y=self.data[idx,1,:,:]\n",
    "        if self.transform:\n",
    "            xs=tuple(self.transform(x) for x in xs)\n",
    "            y=self.transform(y)\n",
    "        return xs, y\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([ #Now composition is unnecessary, but maybe more transforms will be added\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "    \n",
    "train_dataset_raw, test_dataset_raw = train_test_split(triplets, test_size=0.2, shuffle=True)\n",
    "    \n",
    "train_dataset = Triplets(train_dataset_raw, transform=train_transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "test_dataset = Triplets(test_dataset_raw, transform=test_transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 9, padding = 4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 1, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 5, padding = 2)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1, padding = 0)\n",
    "        \n",
    "    def forward(self, x1, x3):\n",
    "        x1 = F.selu(self.conv1(x1))\n",
    "        x1 = F.selu(self.conv2(x1))\n",
    "        x1 = F.selu(self.conv3(x1))\n",
    "        x3 = F.selu(self.conv1(x3))\n",
    "        x3 = F.selu(self.conv2(x3))\n",
    "        x3 = F.selu(self.conv3(x3))\n",
    "        out = torch.cat((x1, x3), dim=1)\n",
    "        out = F.selu(self.conv2(out))\n",
    "        out = F.selu(self.conv3(out))\n",
    "        out = F.selu(self.conv4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,device,optimizer,criterion,train_loader, log_freq=5):\n",
    "    \n",
    "    for batch_idx, (xs, ys) in enumerate(train_loader):\n",
    "        xs=[x.to(device, dtype=torch.float) for x in xs]\n",
    "        ys=ys.to(device, dtype=torch.float)\n",
    "        outputs = model(xs[0],xs[1])\n",
    "        loss = criterion(outputs, ys)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_freq == 0:\n",
    "            print(f\"Batch {batch_idx+1} / {len(train_loader)} | Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,device=device,criterion=nn.SmoothL1Loss(),test_loader=test_loader, plot_result=False, plot_error=True):\n",
    "    #plot_error only applies if plot_result is True\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xs, ys) in enumerate(test_loader):\n",
    "            xs=[x.to(device, dtype=torch.float) for x in xs]\n",
    "            ys=ys.to(device, dtype=torch.float)\n",
    "            pred_tensor = model(xs[0],xs[1])\n",
    "            loss = criterion(pred_tensor, ys)\n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "            if plot_result:\n",
    "                \n",
    "                pred_np=pred_tensor[0,0,:,:].cpu().detach().numpy()\n",
    "                x1_np=xs[0][0,0,:,:].cpu().detach().numpy()\n",
    "                x3_np=xs[1][0,0,:,:].cpu().detach().numpy()\n",
    "                y_np=ys[0,0,:,:].cpu().detach().numpy()\n",
    "                \n",
    "                components=5 if plot_error else 4\n",
    "                fig, axs = plt.subplots(1, components, figsize=(8*components,8))\n",
    "                axs[0].set_title(\"Previous\")\n",
    "                axs[1].set_title(\"Next\")\n",
    "                axs[2].set_title(\"Correct middle\")\n",
    "                axs[3].set_title(\"Predicted middle\")\n",
    "                \n",
    "                axs[0].imshow(x1_np,cmap=plt.cm.Greys_r)\n",
    "                axs[1].imshow(x3_np,cmap=plt.cm.Greys_r)\n",
    "                axs[2].imshow(y_np,cmap=plt.cm.Greys_r)\n",
    "                axs[3].imshow(pred_np,cmap=plt.cm.Greys_r)\n",
    "                \n",
    "                if plot_error:\n",
    "                    axs[4].set_title(\"Error\")\n",
    "                    axs[4].imshow(np.abs(pred_np-y_np),cmap=plt.cm.Greys_r)\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "    avg_loss=total_loss/len(test_loader)\n",
    "    print(f\"The average test loss is {avg_loss}.\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model = ConvNet(), criterion = nn.SmoothL1Loss(), optimizer = optim.SGD,\n",
    "                  learning_rate=learning_rate, number_of_epochs=number_of_epochs,\n",
    "                  train_loader=train_loader, test_loader=test_loader,\n",
    "                  device=device, plot_loss=True):\n",
    "    \n",
    "    model=model.to(device)\n",
    "    optimizer=optimizer(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    best_sofar=None\n",
    "    test_losses=[]\n",
    "    \n",
    "    print(\"Training has started.\")\n",
    "    for epoch in range(number_of_epochs):\n",
    "        print(f\"EPOCH {epoch} HAS STARTED.\")\n",
    "        train_step(model,device,optimizer,criterion,train_loader)\n",
    "        test_loss=evaluate(model,device,criterion,test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        if not best_sofar or test_loss<best_sofar:\n",
    "            best_sofar=test_loss\n",
    "            print(f\"BEST TEST LOSS SO FAR HAS IMPROVED TO {test_loss}!\")\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.figure()\n",
    "        plt.title(\"Test losses\")\n",
    "        plt.plot(test_losses)\n",
    "        plt.show()\n",
    "    print(\"Training has finished.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started.\n",
      "EPOCH 0 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.1074373722076416\n",
      "The average test loss is 0.15692301094532013.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.15692301094532013!\n",
      "EPOCH 1 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.1269174963235855\n",
      "The average test loss is 0.0009865246422123164.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0009865246422123164!\n",
      "EPOCH 2 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.005091967526823282\n",
      "The average test loss is 0.0012632478843443095.\n",
      "EPOCH 3 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.003943233285099268\n",
      "The average test loss is 0.002440175728406757.\n",
      "EPOCH 4 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.004042557440698147\n",
      "The average test loss is 0.0013028183835558592.\n",
      "EPOCH 5 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0030871271155774593\n",
      "The average test loss is 0.0014363025955390185.\n",
      "EPOCH 6 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0033247871324419975\n",
      "The average test loss is 0.0019010055635590105.\n",
      "EPOCH 7 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0028224410489201546\n",
      "The average test loss is 0.0009289317793445661.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0009289317793445661!\n",
      "EPOCH 8 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.002294580452144146\n",
      "The average test loss is 0.0019091446942184121.\n",
      "EPOCH 9 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001953207654878497\n",
      "The average test loss is 0.000948092047474347.\n",
      "EPOCH 10 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.002225384581834078\n",
      "The average test loss is 0.0008345790090970695.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0008345790090970695!\n",
      "EPOCH 11 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.002272319979965687\n",
      "The average test loss is 0.0009191823948640377.\n",
      "EPOCH 12 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0018369221361353993\n",
      "The average test loss is 0.0011731731123290957.\n",
      "EPOCH 13 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001791569171473384\n",
      "The average test loss is 0.000894704824895598.\n",
      "EPOCH 14 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0019952054135501385\n",
      "The average test loss is 0.0010230005776975305.\n",
      "EPOCH 15 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0016379568260163069\n",
      "The average test loss is 0.0013734739157371223.\n",
      "EPOCH 16 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0017620958387851715\n",
      "The average test loss is 0.0010109245660714806.\n",
      "EPOCH 17 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.00194935139734298\n",
      "The average test loss is 0.0007863846345571801.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0007863846345571801!\n",
      "EPOCH 18 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0013521360233426094\n",
      "The average test loss is 0.001058919308707118.\n",
      "EPOCH 19 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011165242176502943\n",
      "The average test loss is 0.0008326440642122179.\n",
      "EPOCH 20 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.00123788311611861\n",
      "The average test loss is 0.0010206549370195717.\n",
      "EPOCH 21 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0013033108552917838\n",
      "The average test loss is 0.0007772904849844053.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0007772904849844053!\n",
      "EPOCH 22 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011437314096838236\n",
      "The average test loss is 0.002387979091145098.\n",
      "EPOCH 23 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0023367623798549175\n",
      "The average test loss is 0.0010734957235399635.\n",
      "EPOCH 24 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0014933906495571136\n",
      "The average test loss is 0.0016529267595615238.\n",
      "EPOCH 25 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0015973594272509217\n",
      "The average test loss is 0.0008053972414927557.\n",
      "EPOCH 26 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009470363729633391\n",
      "The average test loss is 0.0007735707418760285.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0007735707418760285!\n",
      "EPOCH 27 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009809726616367698\n",
      "The average test loss is 0.0009292022587032989.\n",
      "EPOCH 28 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0010295554529875517\n",
      "The average test loss is 0.0007573588110972196.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0007573588110972196!\n",
      "EPOCH 29 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009295972995460033\n",
      "The average test loss is 0.000993923540227115.\n",
      "EPOCH 30 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0010321850422769785\n",
      "The average test loss is 0.0010094489756738768.\n",
      "EPOCH 31 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011886213906109333\n",
      "The average test loss is 0.003256656927987933.\n",
      "EPOCH 32 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.002932763658463955\n",
      "The average test loss is 0.002177214133553207.\n",
      "EPOCH 33 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0023419465869665146\n",
      "The average test loss is 0.0023654733784496786.\n",
      "EPOCH 34 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0022545806132256985\n",
      "The average test loss is 0.001719098212197423.\n",
      "EPOCH 35 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0020205595064908266\n",
      "The average test loss is 0.002177018020302057.\n",
      "EPOCH 36 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0020838326308876276\n",
      "The average test loss is 0.0008057447499595583.\n",
      "EPOCH 37 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009974257554858923\n",
      "The average test loss is 0.0007132755767088383.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0007132755767088383!\n",
      "EPOCH 38 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007731711375527084\n",
      "The average test loss is 0.0008188074076315388.\n",
      "EPOCH 39 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0008631582604721189\n",
      "The average test loss is 0.0006908500246936456.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006908500246936456!\n",
      "EPOCH 40 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0008019983069971204\n",
      "The average test loss is 0.0008075497898971662.\n",
      "EPOCH 41 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.000817004416603595\n",
      "The average test loss is 0.0006849647528724745.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006849647528724745!\n",
      "EPOCH 42 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.000775812310166657\n",
      "The average test loss is 0.0010187418258283288.\n",
      "EPOCH 43 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001035048859193921\n",
      "The average test loss is 0.00114023644127883.\n",
      "EPOCH 44 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011927050072699785\n",
      "The average test loss is 0.0019229108467698098.\n",
      "EPOCH 45 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0017167363548651338\n",
      "The average test loss is 0.005633223382756114.\n",
      "EPOCH 46 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.005509452894330025\n",
      "The average test loss is 0.0020222405437380075.\n",
      "EPOCH 47 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001801402773708105\n",
      "The average test loss is 0.0006507330108433962.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006507330108433962!\n",
      "EPOCH 48 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007470598793588579\n",
      "The average test loss is 0.0006389712187228724.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006389712187228724!\n",
      "EPOCH 49 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007625708822160959\n",
      "The average test loss is 0.0006970045011257753.\n",
      "EPOCH 50 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0008087672758847475\n",
      "The average test loss is 0.0014533997571561485.\n",
      "EPOCH 51 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0014361392240971327\n",
      "The average test loss is 0.0008474477042909711.\n",
      "EPOCH 52 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009538601152598858\n",
      "The average test loss is 0.0006708136002998799.\n",
      "EPOCH 53 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.000786336837336421\n",
      "The average test loss is 0.0006294134916970506.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006294134916970506!\n",
      "EPOCH 54 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007273998344317079\n",
      "The average test loss is 0.0006340387888485565.\n",
      "EPOCH 55 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0005722247296944261\n",
      "The average test loss is 0.0006069853901863098.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0006069853901863098!\n",
      "EPOCH 56 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0006379021797329187\n",
      "The average test loss is 0.0006630235759075731.\n",
      "EPOCH 57 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0006269334116950631\n",
      "The average test loss is 0.0006685566186206415.\n",
      "EPOCH 58 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007167293806560338\n",
      "The average test loss is 0.0010564840340521187.\n",
      "EPOCH 59 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0008554633241146803\n",
      "The average test loss is 0.001056987454649061.\n",
      "EPOCH 60 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011100585106760263\n",
      "The average test loss is 0.001611711992882192.\n",
      "EPOCH 61 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001432840945199132\n",
      "The average test loss is 0.001470925024477765.\n",
      "EPOCH 62 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0014922937843948603\n",
      "The average test loss is 0.0011849397735204548.\n",
      "EPOCH 63 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009465623297728598\n",
      "The average test loss is 0.0006799007387598976.\n",
      "EPOCH 64 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007463522488251328\n",
      "The average test loss is 0.0011445599608123302.\n",
      "EPOCH 65 HAS STARTED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 / 5 | Loss = 0.0011736174346879125\n",
      "The average test loss is 0.0006665241700829938.\n",
      "EPOCH 66 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007444584043696523\n",
      "The average test loss is 0.001769565202994272.\n",
      "EPOCH 67 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.002002416644245386\n",
      "The average test loss is 0.0014164256164804101.\n",
      "EPOCH 68 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0015602586790919304\n",
      "The average test loss is 0.0026786842034198345.\n",
      "EPOCH 69 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0024228375405073166\n",
      "The average test loss is 0.001116247067693621.\n",
      "EPOCH 70 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.001206763437949121\n",
      "The average test loss is 0.0012767803214956074.\n",
      "EPOCH 71 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0011675090063363314\n",
      "The average test loss is 0.0008876104955561459.\n",
      "EPOCH 72 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0009626551764085889\n",
      "The average test loss is 0.0006478644354501739.\n",
      "EPOCH 73 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007153823389671743\n",
      "The average test loss is 0.0007465124916052445.\n",
      "EPOCH 74 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0008340136846527457\n",
      "The average test loss is 0.0008019762055482716.\n",
      "EPOCH 75 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007786314818076789\n",
      "The average test loss is 0.0005310534528689459.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0005310534528689459!\n",
      "EPOCH 76 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0006008925847709179\n",
      "The average test loss is 0.0005442799534648657.\n",
      "EPOCH 77 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0006598285981453955\n",
      "The average test loss is 0.0005453875550301745.\n",
      "EPOCH 78 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0005742011126130819\n",
      "The average test loss is 0.0006051374395610765.\n",
      "EPOCH 79 HAS STARTED.\n",
      "Batch 1 / 5 | Loss = 0.0007012238493189216\n",
      "The average test loss is 0.0005259082216070964.\n",
      "BEST TEST LOSS SO FAR HAS IMPROVED TO 0.0005259082216070964!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has finished.\n"
     ]
    }
   ],
   "source": [
    "trained_model=train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number=1\n",
    "base_models_path=\"/mnt/idms/PROJECTS/Lung/LungCT/models\"\n",
    "while os.path.exists(f\"{base_models_path}/{model_number}.pth\"):\n",
    "    model_number+=1\n",
    "torch.save(trained_model, f\"{base_models_path}/{model_number}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_test_loss=evaluate(trained_model,plot_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0328,  0.0926,  0.0409,  ..., -0.0302,  0.0694, -0.0819],\n",
      "          [-0.0489, -0.0259,  0.0007,  ..., -0.0859, -0.0331,  0.0446],\n",
      "          [-0.1056, -0.0402, -0.0662,  ...,  0.0291,  0.0133, -0.0568],\n",
      "          ...,\n",
      "          [-0.0137, -0.0098, -0.0512,  ..., -0.0040,  0.0859, -0.0282],\n",
      "          [-0.0980, -0.0318, -0.1011,  ...,  0.0700,  0.0283, -0.0630],\n",
      "          [ 0.0214,  0.0357, -0.0219,  ..., -0.1061, -0.0088,  0.0601]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0251, -0.0310,  0.0073,  ...,  0.0122,  0.0179, -0.0039],\n",
      "          [ 0.0812, -0.0502,  0.0733,  ...,  0.0019, -0.0797,  0.0367],\n",
      "          [-0.0651,  0.0508,  0.0053,  ..., -0.1055,  0.0425,  0.0181],\n",
      "          ...,\n",
      "          [ 0.0416, -0.1011, -0.0248,  ...,  0.0706,  0.0927, -0.0960],\n",
      "          [ 0.0380,  0.0583,  0.0417,  ..., -0.0415,  0.0653,  0.0991],\n",
      "          [ 0.0190,  0.0633, -0.0693,  ...,  0.0359, -0.0435,  0.0596]]],\n",
      "\n",
      "\n",
      "        [[[-0.0175, -0.0332, -0.0883,  ...,  0.0131,  0.0310, -0.0238],\n",
      "          [-0.0805,  0.0732, -0.0639,  ..., -0.0838, -0.0167,  0.0442],\n",
      "          [ 0.0294,  0.0578, -0.1051,  ..., -0.0493, -0.0068,  0.0298],\n",
      "          ...,\n",
      "          [ 0.0866,  0.0168, -0.1007,  ..., -0.0416, -0.0487, -0.1076],\n",
      "          [-0.1023,  0.0482, -0.0877,  ...,  0.0450,  0.0181, -0.0412],\n",
      "          [ 0.1112,  0.1078,  0.0421,  ..., -0.0680,  0.0184, -0.1069]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0306, -0.0258, -0.0300,  ...,  0.0046, -0.0732,  0.0325],\n",
      "          [ 0.0471, -0.0170, -0.0394,  ...,  0.0014,  0.0663,  0.0924],\n",
      "          [-0.0195,  0.1048, -0.0050,  ...,  0.0829, -0.0958,  0.0877],\n",
      "          ...,\n",
      "          [-0.0621, -0.0769, -0.0120,  ..., -0.1073, -0.1000,  0.0972],\n",
      "          [ 0.0619,  0.0104, -0.0880,  ..., -0.0880, -0.0626,  0.0210],\n",
      "          [ 0.0758,  0.0223,  0.0343,  ..., -0.0496,  0.0186,  0.0868]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0414, -0.0264, -0.0440,  ...,  0.0314,  0.0382,  0.0200],\n",
      "          [-0.0502, -0.0549, -0.1034,  ..., -0.0838,  0.0813,  0.0006],\n",
      "          [ 0.1115,  0.1098, -0.0515,  ...,  0.0333,  0.0672, -0.0384],\n",
      "          ...,\n",
      "          [-0.0164,  0.0867,  0.0061,  ...,  0.0341,  0.0220, -0.0043],\n",
      "          [ 0.0110,  0.0059,  0.0382,  ..., -0.0944,  0.0624, -0.0359],\n",
      "          [ 0.0332,  0.1070,  0.0732,  ..., -0.0314, -0.0608, -0.0220]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0904,  0.0389, -0.0961,  ...,  0.0986,  0.0652, -0.0092],\n",
      "          [ 0.0123,  0.0881, -0.1036,  ...,  0.0235,  0.0413,  0.0477],\n",
      "          [ 0.0894, -0.0031,  0.0722,  ..., -0.0254,  0.0023,  0.0570],\n",
      "          ...,\n",
      "          [ 0.0118, -0.0436, -0.0647,  ..., -0.0178, -0.0057, -0.0202],\n",
      "          [ 0.0297,  0.0752, -0.0102,  ...,  0.0394, -0.0338, -0.0600],\n",
      "          [-0.0978, -0.0621, -0.1022,  ...,  0.0513, -0.0927, -0.0362]]]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0113,  0.0724,  0.0686,  0.0849,  0.0589,  0.0432,  0.0168,  0.0575,\n",
      "        -0.0508, -0.0862, -0.0452,  0.0770,  0.0808,  0.0857,  0.0027,  0.0357,\n",
      "         0.0408, -0.0663, -0.0331, -0.0426, -0.0988,  0.0030, -0.0517, -0.0916,\n",
      "         0.0891,  0.0372, -0.1092,  0.0642, -0.0163, -0.0175, -0.0793, -0.0409,\n",
      "         0.0869, -0.0741,  0.0271,  0.1163, -0.0093, -0.0141, -0.0692, -0.0378,\n",
      "        -0.0119,  0.0724,  0.0527, -0.0683,  0.1072,  0.0902,  0.0266, -0.0245,\n",
      "         0.0016,  0.0579,  0.0826, -0.0451,  0.0675, -0.0153, -0.0875, -0.1122,\n",
      "        -0.1057,  0.0305, -0.0656, -0.1035, -0.0418, -0.0317,  0.0193, -0.0977],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.0806]],\n",
      "\n",
      "         [[-0.0605]],\n",
      "\n",
      "         [[ 0.0792]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0771]],\n",
      "\n",
      "         [[-0.0183]],\n",
      "\n",
      "         [[ 0.0695]]],\n",
      "\n",
      "\n",
      "        [[[-0.0799]],\n",
      "\n",
      "         [[-0.0833]],\n",
      "\n",
      "         [[-0.1044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0245]],\n",
      "\n",
      "         [[ 0.0268]],\n",
      "\n",
      "         [[-0.0225]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0695]],\n",
      "\n",
      "         [[-0.1202]],\n",
      "\n",
      "         [[-0.0386]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1176]],\n",
      "\n",
      "         [[-0.0872]],\n",
      "\n",
      "         [[ 0.1175]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0102]],\n",
      "\n",
      "         [[-0.0922]],\n",
      "\n",
      "         [[-0.0668]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0686]],\n",
      "\n",
      "         [[ 0.1088]],\n",
      "\n",
      "         [[ 0.1076]]],\n",
      "\n",
      "\n",
      "        [[[-0.0725]],\n",
      "\n",
      "         [[ 0.0315]],\n",
      "\n",
      "         [[ 0.0596]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0448]],\n",
      "\n",
      "         [[-0.1012]],\n",
      "\n",
      "         [[ 0.0581]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0721]],\n",
      "\n",
      "         [[ 0.0489]],\n",
      "\n",
      "         [[-0.0417]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0478]],\n",
      "\n",
      "         [[-0.0964]],\n",
      "\n",
      "         [[ 0.0185]]]], device='cuda:0')\n",
      "tensor([-0.0867, -0.0209, -0.0900, -0.0507, -0.0516, -0.0806, -0.0693,  0.1063,\n",
      "        -0.0532, -0.0177,  0.0305,  0.0569, -0.0076, -0.0005, -0.0814, -0.0199,\n",
      "         0.0890,  0.0266,  0.0996,  0.0725,  0.0071, -0.0966,  0.0991,  0.0606,\n",
      "        -0.0812,  0.0661,  0.0931, -0.0850,  0.0138,  0.0735,  0.0639, -0.0938],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 8.9514e-04, -2.1845e-02, -7.4351e-03, -3.2209e-02,  1.4505e-02],\n",
      "          [ 1.7533e-02, -1.2974e-02, -1.2038e-03,  3.3241e-02,  3.0436e-02],\n",
      "          [-2.9935e-02, -2.2383e-02,  5.8793e-03,  3.6534e-02, -2.7907e-02],\n",
      "          [-2.5873e-02,  2.4131e-02,  2.3685e-03, -2.5566e-02,  2.9962e-02],\n",
      "          [-1.3666e-02, -1.9872e-02, -2.4128e-02,  2.7410e-02,  1.8033e-02]],\n",
      "\n",
      "         [[ 1.1963e-03,  2.5622e-02,  1.3259e-02,  1.0621e-02, -3.4136e-02],\n",
      "          [-3.5924e-03, -1.1896e-02,  5.0286e-04, -2.2981e-03,  7.4652e-03],\n",
      "          [-3.1600e-02, -2.6653e-02,  2.8149e-02, -2.7912e-03, -8.6270e-03],\n",
      "          [ 6.2843e-03, -2.3223e-02, -1.0913e-02,  3.1750e-02, -2.0419e-02],\n",
      "          [-2.5795e-02,  1.1293e-02,  1.5214e-04,  2.2680e-02,  2.3479e-03]],\n",
      "\n",
      "         [[-2.6044e-02, -8.0660e-03,  3.0427e-04, -1.7015e-02,  3.7189e-03],\n",
      "          [-3.8470e-02, -5.6112e-05, -1.5116e-02,  1.6182e-02, -1.1388e-03],\n",
      "          [ 1.9974e-02, -9.3954e-03,  2.6798e-02, -3.1848e-02, -3.4299e-02],\n",
      "          [-5.1574e-03, -2.1736e-02, -2.8574e-02,  2.8058e-02, -3.5659e-02],\n",
      "          [ 2.8514e-02,  3.1083e-02,  2.8914e-02, -3.3911e-02, -1.3740e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9873e-02, -3.3091e-02, -3.3957e-02,  1.9020e-02, -3.2403e-02],\n",
      "          [-2.4226e-02, -2.5137e-02,  1.5923e-03,  2.4912e-02, -1.9792e-02],\n",
      "          [ 2.7524e-02, -1.5829e-02,  1.3844e-02, -1.1950e-02,  2.4697e-02],\n",
      "          [-2.1768e-02, -1.0329e-02,  1.0292e-02, -2.0807e-02, -2.6996e-02],\n",
      "          [-5.1547e-04,  4.4799e-03, -2.1267e-02,  1.9999e-02,  8.7549e-03]],\n",
      "\n",
      "         [[ 2.4634e-02, -2.1785e-03, -1.7628e-03,  3.6216e-02,  2.2254e-03],\n",
      "          [-8.0897e-03, -2.6988e-02, -1.9203e-02, -1.7687e-02, -3.1496e-02],\n",
      "          [ 2.3210e-02,  3.3454e-02,  1.2789e-02,  1.4462e-02,  1.4270e-02],\n",
      "          [ 6.6888e-04,  1.6434e-02,  2.1949e-03, -1.8586e-02, -1.6458e-02],\n",
      "          [ 2.0249e-02, -2.9749e-02,  1.8827e-02, -1.2117e-02,  1.0716e-02]],\n",
      "\n",
      "         [[-5.7870e-03, -2.1052e-02, -2.7234e-02,  2.4061e-02, -2.4483e-02],\n",
      "          [-3.1947e-02,  1.2054e-02, -3.2452e-02, -3.0958e-02, -3.0718e-02],\n",
      "          [-1.8472e-02,  1.2816e-02,  2.2741e-02,  2.0881e-02,  3.6295e-02],\n",
      "          [-2.4025e-02,  6.7774e-03,  2.5603e-02, -2.9491e-02, -1.6984e-03],\n",
      "          [-1.6411e-02,  2.6142e-02,  1.8303e-02,  4.4260e-03,  2.1101e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.0058e-02, -1.8113e-02,  3.6340e-03,  2.6451e-03, -3.6789e-02],\n",
      "          [-2.2564e-02, -1.3006e-02,  1.4920e-02, -2.4655e-02,  1.9707e-02],\n",
      "          [ 1.7826e-02, -1.6792e-02,  2.0497e-02, -1.8085e-02, -3.7374e-02],\n",
      "          [-8.7948e-03,  2.1796e-02, -3.9300e-02, -2.0344e-02,  2.5040e-03],\n",
      "          [ 1.2505e-02,  9.7772e-03, -2.1742e-02, -7.9128e-03, -2.6406e-02]],\n",
      "\n",
      "         [[-3.0087e-02, -1.1584e-02, -2.3207e-02, -1.1178e-02,  3.8908e-02],\n",
      "          [-1.7445e-03, -1.8518e-02, -1.7960e-02,  1.5621e-02, -3.0409e-02],\n",
      "          [-2.1450e-02,  3.5291e-02, -1.0936e-02, -2.1767e-02, -3.1277e-02],\n",
      "          [-1.5228e-03,  1.6699e-02,  2.9403e-02,  2.0067e-02,  8.2566e-03],\n",
      "          [-8.5913e-03, -3.4092e-03,  1.7346e-02, -1.2119e-02,  3.0716e-02]],\n",
      "\n",
      "         [[-3.5056e-02, -2.5469e-02,  2.0119e-02, -2.6205e-02, -3.4063e-02],\n",
      "          [-3.0711e-02,  2.2829e-02,  1.6410e-02, -3.4395e-03, -1.7331e-02],\n",
      "          [ 2.2763e-02, -3.1470e-02,  7.2585e-03, -7.1162e-03,  5.6598e-03],\n",
      "          [-2.4399e-02,  4.9053e-03, -3.1312e-02, -3.0404e-02, -2.0962e-02],\n",
      "          [-7.6159e-03,  2.2230e-03, -1.0738e-02, -2.9726e-02, -1.0706e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5196e-02, -1.1985e-02, -5.8662e-03, -2.1197e-02, -1.7362e-02],\n",
      "          [ 3.8490e-02,  1.3171e-02, -1.5400e-02, -9.2184e-03, -1.1263e-02],\n",
      "          [-4.8981e-03, -2.6868e-02,  2.9294e-02,  2.2938e-02,  3.5232e-02],\n",
      "          [ 1.1669e-02,  2.7786e-03,  3.6690e-02, -3.0182e-02,  7.4887e-03],\n",
      "          [ 1.5734e-02, -2.5008e-03, -2.5577e-02,  3.7277e-03,  2.5431e-02]],\n",
      "\n",
      "         [[-3.2115e-02, -3.4517e-03,  2.0588e-03, -1.4199e-02, -3.9167e-03],\n",
      "          [ 2.7839e-03, -8.7916e-03,  5.0931e-03,  1.6016e-02, -2.8132e-02],\n",
      "          [ 2.1076e-02, -2.2965e-03, -3.3452e-02, -9.8901e-03, -9.3620e-03],\n",
      "          [ 1.7934e-02, -3.2450e-02,  2.7844e-02, -1.1099e-02,  1.0121e-02],\n",
      "          [-3.2575e-02,  1.2908e-02, -1.6629e-02,  2.0651e-02,  6.4855e-03]],\n",
      "\n",
      "         [[-1.0054e-02,  2.5034e-02,  2.1321e-02, -7.3374e-03, -2.7429e-02],\n",
      "          [-1.9649e-02,  1.9566e-02, -7.3137e-03,  2.0262e-02, -7.0498e-03],\n",
      "          [-2.4921e-02,  1.6784e-03,  2.5091e-02, -1.6917e-02,  9.8626e-03],\n",
      "          [ 8.0042e-03,  1.8406e-02, -1.6835e-03, -5.7101e-03, -2.3321e-02],\n",
      "          [ 2.1755e-02, -6.8668e-04,  9.6651e-03, -4.5376e-03, -1.2269e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0947e-02,  8.6765e-03, -2.3636e-02, -3.4046e-02, -1.2988e-02],\n",
      "          [ 1.0178e-02, -8.7068e-03, -7.4073e-03, -3.1787e-02,  1.9766e-02],\n",
      "          [ 2.8628e-02,  5.7779e-03,  2.4159e-02,  7.1279e-03, -1.9718e-02],\n",
      "          [-3.9873e-03, -4.1909e-03,  2.5276e-03,  1.0069e-02,  1.8047e-02],\n",
      "          [-2.5282e-02,  1.3323e-02, -2.8672e-02,  1.8997e-03, -2.8333e-02]],\n",
      "\n",
      "         [[-5.5925e-03, -3.8825e-03,  2.8597e-02, -2.2452e-02,  2.6018e-02],\n",
      "          [ 6.4042e-03,  3.0250e-02,  2.8512e-02,  1.7771e-02,  1.6073e-02],\n",
      "          [ 1.4306e-02, -1.5020e-02, -3.4004e-02, -1.2026e-02, -2.1944e-02],\n",
      "          [-3.4803e-02, -2.1589e-02, -3.1150e-02,  1.1708e-02,  2.2429e-02],\n",
      "          [ 3.3018e-02, -2.8795e-02,  2.7260e-02, -3.4713e-02,  7.6522e-03]],\n",
      "\n",
      "         [[-1.2040e-02, -2.4384e-02, -9.1374e-03,  2.2286e-02, -2.4417e-02],\n",
      "          [-2.3303e-02,  1.9265e-02,  2.4964e-02, -1.6160e-03,  1.8110e-02],\n",
      "          [ 2.7029e-02,  2.8138e-02, -1.0176e-02,  1.4281e-02,  2.2650e-02],\n",
      "          [ 1.8094e-02, -2.0847e-02, -2.4461e-02,  7.6228e-03, -1.9562e-02],\n",
      "          [ 4.5709e-03,  6.7850e-03, -3.0927e-03, -8.1893e-03,  3.4081e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0640e-03, -2.9048e-02,  1.8780e-02, -3.0827e-02, -2.1568e-02],\n",
      "          [-2.3204e-02,  5.6525e-03,  2.6461e-02, -2.2803e-02, -1.4021e-02],\n",
      "          [ 3.3803e-02, -1.3552e-03, -2.6819e-03, -6.6507e-03, -1.6001e-02],\n",
      "          [-6.1182e-03, -3.2619e-03, -3.0483e-03,  2.3360e-02, -1.2232e-02],\n",
      "          [ 1.9988e-03,  1.2990e-02,  2.1382e-02,  1.1878e-02,  1.0763e-02]],\n",
      "\n",
      "         [[-9.4689e-03,  2.0484e-02, -1.1586e-02,  7.8702e-03, -2.8149e-02],\n",
      "          [-1.8481e-02,  3.1626e-02, -2.1625e-02,  2.4797e-02,  3.2235e-02],\n",
      "          [-9.0341e-03,  9.0499e-03, -5.6813e-04,  3.8479e-04,  8.5712e-03],\n",
      "          [-2.7152e-02,  2.5543e-02,  3.0523e-02,  1.5932e-02, -3.2146e-02],\n",
      "          [-1.9883e-02,  2.0510e-02,  8.3981e-03,  1.0673e-02, -3.6343e-02]],\n",
      "\n",
      "         [[-7.0860e-03, -1.4102e-02,  1.7754e-02,  2.4690e-02, -2.6469e-02],\n",
      "          [-8.6984e-03,  6.0315e-04,  6.1778e-03,  1.3495e-02,  3.3688e-02],\n",
      "          [ 2.2337e-02, -2.8346e-02,  1.7701e-02,  3.5300e-03, -2.7280e-02],\n",
      "          [-8.6855e-03,  1.8431e-02, -3.2314e-02,  1.1634e-02,  2.2304e-02],\n",
      "          [ 3.2243e-02,  3.1388e-02,  2.6708e-02, -3.3294e-02, -3.2794e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2072e-02,  1.9562e-02,  9.0604e-03,  1.9123e-02,  1.1126e-02],\n",
      "          [-2.8383e-02, -4.1703e-03, -3.6730e-02,  5.1427e-03,  2.3512e-02],\n",
      "          [ 2.5536e-04, -1.5576e-02, -3.4276e-02, -8.6717e-03, -1.0407e-02],\n",
      "          [-3.0609e-02,  2.2977e-02, -2.2879e-02,  2.5061e-03,  1.4079e-02],\n",
      "          [-2.2746e-03, -2.8677e-03,  2.6391e-02, -3.3175e-02, -1.7282e-02]],\n",
      "\n",
      "         [[ 1.0592e-02, -2.0821e-02,  9.5728e-03,  4.5500e-03, -2.0701e-02],\n",
      "          [-2.2366e-02, -5.4122e-04,  3.5896e-03,  3.1561e-02,  5.9976e-05],\n",
      "          [ 1.4368e-02,  8.6320e-03, -8.0343e-04,  1.2629e-02,  3.5430e-02],\n",
      "          [-2.8719e-02,  2.9572e-02, -3.1915e-02, -1.8410e-02,  2.2653e-02],\n",
      "          [-2.4286e-02, -2.5878e-02,  3.3304e-02, -1.5105e-02,  1.5014e-02]],\n",
      "\n",
      "         [[ 3.0621e-02, -4.7794e-04,  2.7824e-02, -1.7383e-02, -1.7364e-02],\n",
      "          [ 9.4301e-03, -2.9178e-02, -7.3325e-03, -2.1760e-02, -3.4106e-02],\n",
      "          [ 2.7389e-02,  1.8248e-02, -2.2059e-02, -5.9784e-03, -7.8689e-03],\n",
      "          [-5.4572e-04, -2.7346e-02, -3.0045e-02, -3.4302e-02,  1.6911e-02],\n",
      "          [-4.0587e-03, -1.7631e-02, -3.1683e-02, -3.0304e-02, -2.8432e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8931e-03, -2.0782e-02, -1.2011e-02,  3.3538e-02, -2.8592e-02],\n",
      "          [-2.3432e-02,  4.4314e-03,  3.0330e-02,  1.0013e-02,  1.6255e-02],\n",
      "          [-1.1494e-02, -1.6256e-02,  3.2267e-02,  7.8498e-03, -2.3574e-02],\n",
      "          [ 3.0390e-02,  2.4067e-02,  2.3390e-02, -3.1957e-02, -1.1746e-02],\n",
      "          [-6.9349e-04,  1.4501e-03,  3.4389e-04, -3.9506e-03, -1.2911e-02]],\n",
      "\n",
      "         [[ 3.3857e-02, -1.2928e-02,  1.2711e-03,  3.0371e-03, -1.6394e-02],\n",
      "          [ 2.0906e-02,  2.9560e-02,  1.8368e-02, -2.1201e-02,  9.2089e-03],\n",
      "          [ 6.1970e-03,  6.3290e-03,  3.8410e-03, -1.6660e-03,  3.3589e-02],\n",
      "          [-8.7448e-03,  2.7052e-02, -2.4550e-02, -2.3312e-03,  1.3946e-02],\n",
      "          [-2.3029e-02,  2.6165e-02,  1.6806e-04, -2.3841e-02,  1.6897e-02]],\n",
      "\n",
      "         [[-8.3218e-03, -9.9633e-03,  1.7411e-02, -3.4006e-02,  2.3299e-02],\n",
      "          [-6.8086e-04, -1.7140e-02, -2.4920e-02,  1.6444e-02, -2.3199e-02],\n",
      "          [-2.6182e-03,  2.4873e-02,  1.1591e-03, -1.4983e-02, -1.6207e-02],\n",
      "          [ 1.4774e-02, -1.6426e-02, -2.9469e-02,  8.4189e-03, -3.5796e-02],\n",
      "          [ 2.6677e-02,  3.1351e-02, -3.4270e-03,  1.6705e-02, -2.4352e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1394e-02, -1.7453e-02,  2.5657e-02, -3.5216e-02, -1.0500e-02],\n",
      "          [-7.0591e-03, -4.7695e-03, -1.2260e-03,  1.2207e-02,  1.7891e-02],\n",
      "          [-7.2036e-03,  2.5681e-02,  8.6366e-03, -3.1872e-02, -3.2689e-03],\n",
      "          [-3.5877e-02,  5.9397e-03, -2.2490e-02,  2.4700e-02, -1.2955e-02],\n",
      "          [ 1.6209e-02, -3.1308e-02,  2.0741e-02,  2.8365e-02,  2.0995e-02]],\n",
      "\n",
      "         [[ 3.0930e-02,  2.5945e-02, -1.4649e-02, -2.7082e-02, -1.2483e-02],\n",
      "          [-2.0885e-02, -1.0774e-02, -4.6844e-03,  2.9197e-02,  1.9093e-02],\n",
      "          [ 3.3216e-03, -1.1101e-02, -2.1980e-02,  1.6384e-02,  2.0340e-03],\n",
      "          [-2.2015e-02, -2.9755e-02,  1.8798e-02,  2.9874e-02,  3.4690e-02],\n",
      "          [-8.2635e-03,  7.0441e-03,  2.3221e-02, -2.4106e-03,  1.8020e-02]],\n",
      "\n",
      "         [[-3.5001e-02, -1.1217e-02,  1.6245e-02, -2.3907e-02, -5.0034e-03],\n",
      "          [-1.8420e-02, -2.3663e-02, -1.8159e-02,  2.1156e-02, -3.1449e-02],\n",
      "          [ 2.4478e-02,  2.3692e-02, -2.3309e-02, -1.1257e-02,  9.1209e-03],\n",
      "          [-1.6555e-02,  1.5148e-02,  2.8035e-02, -9.4401e-03, -3.2324e-02],\n",
      "          [-1.8225e-03,  1.0433e-02,  5.4366e-03, -1.8476e-02,  2.1717e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3489e-02,  1.2500e-02,  1.7869e-02, -7.1476e-03, -1.7985e-02],\n",
      "          [-2.4748e-02, -3.3493e-02, -2.9245e-02,  2.2465e-02,  1.2445e-02],\n",
      "          [-1.1968e-02,  2.4234e-02,  1.2396e-02, -2.8674e-02,  2.1718e-02],\n",
      "          [ 1.7535e-02, -7.9728e-03, -1.6226e-02, -6.6028e-03, -3.0378e-02],\n",
      "          [ 1.0788e-02,  1.0459e-02, -1.9206e-02, -3.0403e-02,  1.7949e-02]],\n",
      "\n",
      "         [[-1.1367e-02, -6.8521e-03,  1.4362e-02,  3.6640e-02,  3.7296e-02],\n",
      "          [-9.2083e-03, -2.5133e-02,  1.8721e-02,  7.5785e-03, -1.5998e-02],\n",
      "          [ 2.3306e-02,  2.3244e-02, -8.2155e-03,  1.3005e-03, -3.1500e-02],\n",
      "          [-1.3948e-02,  1.9778e-02, -3.0792e-02, -2.0513e-02, -7.7491e-03],\n",
      "          [-1.4828e-02,  2.5459e-02,  3.7135e-02,  1.6719e-03, -2.6252e-02]],\n",
      "\n",
      "         [[ 9.8421e-03, -3.0268e-02,  7.9921e-03,  3.1527e-02,  2.2983e-02],\n",
      "          [-1.8050e-02,  6.6472e-03,  3.5296e-03, -2.6179e-02,  1.7671e-02],\n",
      "          [ 3.0703e-02, -2.2004e-02, -8.0379e-03,  2.6963e-02,  3.1548e-02],\n",
      "          [ 1.4648e-02,  1.2289e-02, -1.9621e-02,  5.0111e-03, -9.4528e-03],\n",
      "          [ 1.8668e-03,  1.8379e-02, -6.1941e-03,  1.3974e-02,  1.5817e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.1605e-03, -1.1508e-02,  2.6147e-02,  2.1202e-02, -5.2972e-03],\n",
      "          [-3.5353e-03, -7.8115e-03, -3.2021e-02, -7.3418e-03, -1.4265e-02],\n",
      "          [-1.9845e-02,  2.8730e-02, -2.6608e-02,  8.1113e-03,  1.5143e-03],\n",
      "          [ 2.9065e-02, -3.2847e-02,  1.3536e-02,  2.8126e-04,  3.2006e-02],\n",
      "          [ 1.2792e-02,  2.2401e-02,  2.4211e-02, -1.9364e-02, -2.5615e-03]],\n",
      "\n",
      "         [[ 2.2796e-02, -1.2062e-02,  3.1073e-03,  1.9602e-02, -1.2661e-02],\n",
      "          [ 1.4780e-02,  5.5556e-03, -6.5108e-03, -2.7414e-02,  1.1922e-02],\n",
      "          [ 1.9380e-03, -3.6945e-03, -1.6028e-02,  5.3449e-03,  2.0474e-02],\n",
      "          [-2.0637e-02, -5.4323e-03, -1.9480e-02,  3.6961e-02,  3.5062e-02],\n",
      "          [ 1.1622e-02, -1.9962e-02, -6.1345e-03, -2.0169e-02, -3.0967e-02]],\n",
      "\n",
      "         [[ 4.9549e-03,  1.3455e-02, -3.7008e-02, -2.3386e-02, -6.4968e-03],\n",
      "          [-2.1980e-02, -3.0079e-02, -1.7390e-02,  1.6684e-02,  1.7489e-02],\n",
      "          [ 3.1557e-02, -2.3054e-02,  2.2663e-02, -1.6945e-02,  4.7349e-03],\n",
      "          [ 7.3584e-04,  2.4610e-02, -1.4166e-02,  1.4205e-02, -3.1019e-02],\n",
      "          [ 2.7324e-02, -9.5166e-03,  3.0770e-04,  2.3435e-02, -1.7359e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0401e-02, -1.0263e-02,  1.2738e-02,  3.3928e-03,  3.1142e-02],\n",
      "          [-1.9383e-02, -1.8515e-02,  9.6884e-03, -6.9535e-03, -5.2784e-03],\n",
      "          [-2.0568e-02,  3.5080e-02, -1.5860e-02,  1.8706e-02, -4.0648e-03],\n",
      "          [-3.8903e-03,  2.6323e-02,  6.4597e-03, -1.8951e-02,  4.0174e-04],\n",
      "          [-1.6311e-03, -3.3385e-02, -7.1274e-03,  5.5372e-04, -3.0911e-02]],\n",
      "\n",
      "         [[-2.0148e-02, -5.2482e-03, -1.6322e-02, -1.4408e-02,  2.4421e-02],\n",
      "          [ 2.7243e-02, -1.1482e-02, -3.1865e-03,  6.2050e-03, -2.2362e-02],\n",
      "          [-2.0269e-02, -1.5366e-02,  2.9417e-02,  2.7116e-02,  3.0890e-02],\n",
      "          [-3.6040e-02, -2.2460e-02,  1.7082e-02,  2.3904e-02, -9.1062e-03],\n",
      "          [ 2.6269e-03, -2.0570e-02, -2.5194e-02,  2.1057e-02, -2.4809e-02]],\n",
      "\n",
      "         [[ 2.1346e-02,  1.3366e-02,  2.3591e-02, -3.2678e-02, -3.4310e-02],\n",
      "          [-6.6975e-04, -3.2171e-02,  1.1385e-02, -2.3408e-02, -2.6982e-02],\n",
      "          [ 1.5958e-02, -6.0347e-03, -1.5444e-02,  2.7361e-02, -2.2081e-02],\n",
      "          [-1.6639e-02, -2.8589e-02, -6.8362e-03,  3.0421e-02, -1.5458e-02],\n",
      "          [-2.7769e-02,  2.3712e-02, -1.8921e-02,  6.2169e-03,  2.1445e-02]]]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0076,  0.0635,  0.0337,  0.0074,  0.0535,  0.0075, -0.0035, -0.0233,\n",
      "         0.0185, -0.0077,  0.0137, -0.0033,  0.0150,  0.0505,  0.0157, -0.0074,\n",
      "         0.0173, -0.0193, -0.0190,  0.0153, -0.0171,  0.0441, -0.0229,  0.0071,\n",
      "        -0.0092,  0.0394,  0.0025, -0.0082, -0.0031,  0.0346,  0.0409,  0.0322],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.0805]],\n",
      "\n",
      "         [[ 0.1020]],\n",
      "\n",
      "         [[ 0.0534]],\n",
      "\n",
      "         [[-0.0769]],\n",
      "\n",
      "         [[ 0.0989]],\n",
      "\n",
      "         [[ 0.0783]],\n",
      "\n",
      "         [[ 0.0462]],\n",
      "\n",
      "         [[ 0.0657]],\n",
      "\n",
      "         [[ 0.0081]],\n",
      "\n",
      "         [[-0.0660]],\n",
      "\n",
      "         [[-0.0275]],\n",
      "\n",
      "         [[ 0.1320]],\n",
      "\n",
      "         [[ 0.0345]],\n",
      "\n",
      "         [[ 0.1040]],\n",
      "\n",
      "         [[ 0.0415]],\n",
      "\n",
      "         [[-0.1245]],\n",
      "\n",
      "         [[-0.0739]],\n",
      "\n",
      "         [[ 0.0776]],\n",
      "\n",
      "         [[ 0.0586]],\n",
      "\n",
      "         [[ 0.0617]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0699]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0529]],\n",
      "\n",
      "         [[ 0.0864]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0280]],\n",
      "\n",
      "         [[ 0.1110]],\n",
      "\n",
      "         [[ 0.1022]],\n",
      "\n",
      "         [[ 0.0998]],\n",
      "\n",
      "         [[ 0.0266]]]], device='cuda:0')\n",
      "tensor([0.1241], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in trained_model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
